# Logistic Regression Gradient Descent

recap:
* $z=w^Tx+b$
* $\hat{y}=a=\sigma{(z)}$ - prediction
* $L(a,y)=-(y\log(a)+(1-y)log(1-a))$ - loss of one example
