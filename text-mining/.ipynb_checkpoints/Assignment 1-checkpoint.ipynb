{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment, you'll be working with messy medical data and using regex to extract relevant infromation from the data. \n",
    "\n",
    "Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats.\n",
    "\n",
    "The goal of this assignment is to correctly identify all of the different date variants encoded in this dataset and to properly normalize and sort the dates. \n",
    "\n",
    "Here is a list of some of the variants you might encounter in this dataset:\n",
    "* 04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009;\n",
    "* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "* Feb 2009; Sep 2009; Oct 2010\n",
    "* 6/2008; 12/2009\n",
    "* 2009; 2010\n",
    "\n",
    "Once you have extracted these date patterns from the text, the next step is to sort them in ascending chronological order accoring to the following rules:\n",
    "* Assume all dates in xx/xx/xx format are mm/dd/yy\n",
    "* Assume all dates where year is encoded in only two digits are years from the 1900's (e.g. 1/5/89 is January 5th, 1989)\n",
    "* If the day is missing (e.g. 9/2009), assume it is the first day of the month (e.g. September 1, 2009).\n",
    "* If the month is missing (e.g. 2010), assume it is the first of January of that year (e.g. January 1, 2010).\n",
    "* Watch out for potential typos as this is a raw, real-life derived dataset.\n",
    "\n",
    "With these rules in mind, find the correct date in each note and return a pandas Series in chronological order of the original Series' indices.\n",
    "\n",
    "For example if the original series was this:\n",
    "\n",
    "    0    1999\n",
    "    1    2010\n",
    "    2    1978\n",
    "    3    2015\n",
    "    4    1985\n",
    "\n",
    "Your function should return this:\n",
    "\n",
    "    0    2\n",
    "    1    4\n",
    "    2    0\n",
    "    3    1\n",
    "    4    3\n",
    "\n",
    "Your score will be calculated using [Kendall's tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), a correlation measure for ordinal data.\n",
    "\n",
    "*This function should return a Series of length 500 and dtype int.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9\n",
       "1       84\n",
       "2        2\n",
       "3       53\n",
       "4       28\n",
       "5      474\n",
       "6      153\n",
       "7       13\n",
       "8      129\n",
       "9       98\n",
       "10     111\n",
       "11     225\n",
       "12      31\n",
       "13     171\n",
       "14     191\n",
       "15     486\n",
       "16     335\n",
       "17     415\n",
       "18      36\n",
       "19     323\n",
       "20     405\n",
       "21     422\n",
       "22     375\n",
       "23     380\n",
       "24     345\n",
       "25      57\n",
       "26     481\n",
       "27     436\n",
       "28     104\n",
       "29     299\n",
       "      ... \n",
       "470    320\n",
       "471    383\n",
       "472    244\n",
       "473    286\n",
       "474    480\n",
       "475    431\n",
       "476    279\n",
       "477    198\n",
       "478    381\n",
       "479    463\n",
       "480    366\n",
       "481    255\n",
       "482    439\n",
       "483    401\n",
       "484    475\n",
       "485    257\n",
       "486    152\n",
       "487    235\n",
       "488    464\n",
       "489    253\n",
       "490    231\n",
       "491    427\n",
       "492    141\n",
       "493    186\n",
       "494    161\n",
       "495    413\n",
       "496     25\n",
       "497     39\n",
       "498     40\n",
       "499     71\n",
       "Name: key, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df2=pd.DataFrame(df, columns=['text'])\n",
    "\n",
    "#Search for dates\n",
    "#04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "e1=df2['text'].str.extractall(r'(?P<month>(?:\\d{1,2}))/(?P<day>(?:\\d{1,2}))/(?P<year>(?:\\d{4}|\\d{2}))')\n",
    "e1=e1.loc[pd.to_numeric(e1.day)<=31,:]\n",
    "#Mar-20-2009; Mar 20, 2009; March 20, 2009; Mar. 20, 2009; Mar 20 2009;\n",
    "e2=df2['text'].str.extractall(r'(?P<month2>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z.\\- ]*(?P<day2>(?:\\d{1,2}))[,]* (?P<year2>(?:\\d{4}))')\n",
    "#Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "e3=df2['text'].str.extractall(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{1,2})(st|nd|th)+[, ]?(?:\\d{2,4})')\n",
    "#Feb 2009; Sep 2009; Oct 2010\n",
    "e4=df2['text'].str.extractall(r'(?P<month4>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)) (?P<year4>(?:\\d{2,4}))')\n",
    "#6/2008; 12/2009\n",
    "e5=df2['text'].str.extractall(r'(?P<month5>(?:[1][0-2]|\\d{1}))/(?P<year5>(?:\\d{4}))')\n",
    "#2009; 2010\n",
    "e6=df2['text'].str.extractall(r'(?P<year6>(?:1|2)\\d{3})')\n",
    "#20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "e7=df2['text'].str.extractall(r'(?P<e7>(?P<day7>(?:\\d{1,2}) )?(?P<month7>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z., ]*(?P<year7>(?:\\d{4})))')\n",
    "\n",
    "#merge above\n",
    "df2.index.name='key'\n",
    "e1.index.levels[0].name = 'key'\n",
    "e2.index.levels[0].name = 'key'\n",
    "e3.index.levels[0].name = 'key'\n",
    "e4.index.levels[0].name = 'key'\n",
    "e5.index.levels[0].name = 'key'\n",
    "e6.index.levels[0].name = 'key'\n",
    "e7.index.levels[0].name = 'key'\n",
    "all=df2.merge(e1,how='left',on='key').merge(e2,\n",
    "    how='left',on='key').merge(e3,\n",
    "    how='left',on='key').merge(e4, \n",
    "    how='left',on='key').merge(e5, \n",
    "    how='left',on='key').merge(e6, \n",
    "    how='left',on='key').merge(e7, \n",
    "    how='left',on='key')\n",
    "#impute letter months to numbers\n",
    "month_dict={'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "all.replace(dict(month=month_dict, month2=month_dict,  month4=month_dict, month7=month_dict), inplace=True)\n",
    "\n",
    "#impute missing year, month, and day from e1\n",
    "all.month.fillna(all.month7, inplace=True)\n",
    "all.month.fillna(all.month2, inplace=True)\n",
    "all.month.fillna(all.month4, inplace=True)\n",
    "all.month.fillna(all.month5, inplace=True)\n",
    "all.day.fillna(all.day7, inplace=True)\n",
    "all.day.fillna(all.day2, inplace=True)\n",
    "all.year.fillna(all.year7, inplace=True)\n",
    "all.year.fillna(all.year2, inplace=True)\n",
    "all.year.fillna(all.year4, inplace=True)\n",
    "all.year.fillna(all.year5, inplace=True)\n",
    "all.year.fillna(all.year6, inplace=True)\n",
    "all1=all[['text','month', 'day', 'year']]\n",
    "all2=all1.loc[:,('month', 'day', 'year')].apply(pd.to_numeric)\n",
    "\n",
    "#impute missing month and day with 1\n",
    "values = {'month': 1, 'day': 1}\n",
    "all3=all2.fillna(value=values)\n",
    "\n",
    "#convert 2 letter year to 1990's\n",
    "all3.year=np.where(all3.year<1900,all3.year+1900,all3.year)\n",
    "\n",
    "#sort by date\n",
    "all3=all3.sort_values(by=['year','month','day'],na_position='last')\n",
    "all4=all3.drop(['month','day','year'],axis=1).reset_index()\n",
    "pd.Series(all4['key'], index=all4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    df2=pd.DataFrame(df, columns=['text'])\n",
    "\n",
    "    #Search for dates\n",
    "    #04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "    e1=df2['text'].str.extractall(r'(?P<month>(?:\\d{1,2}))/(?P<day>(?:\\d{1,2}))/(?P<year>(?:\\d{4}|\\d{2}))')\n",
    "    e1=e1.loc[pd.to_numeric(e1.day)<=31,:]\n",
    "    #Mar-20-2009; Mar 20, 2009; March 20, 2009; Mar. 20, 2009; Mar 20 2009;\n",
    "    e2=df2['text'].str.extractall(r'(?P<month2>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z.,\\- ]*(?P<day2>(?:\\d{1,2}))[,]* (?P<year2>(?:\\d{4}))')\n",
    "    #Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "    e3=df2['text'].str.extractall(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{1,2})(st|nd|th)+[, ]?(?:\\d{2,4})')\n",
    "    #Feb 2009; Sep 2009; Oct 2010\n",
    "    e4=df2['text'].str.extractall(r'(?P<month4>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)) (?P<year4>(?:\\d{2,4}))')\n",
    "    #6/2008; 12/2009\n",
    "    e5=df2['text'].str.extractall(r'(?P<month5>(?:[1][0-2]|\\d{1}))/(?P<year5>(?:\\d{4}))')\n",
    "    #2009; 2010\n",
    "    e6=df2['text'].str.extractall(r'(?P<year6>(?:1|2)\\d{3})')\n",
    "    #20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "    e7=df2['text'].str.extractall(r'(?P<e7>(?P<day7>(?:\\d{1,2}) )?(?P<month7>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z., ]*(?P<year7>(?:\\d{4})))')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    e1=e1.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e2=e2.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e4=e4.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e5=e5.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e6=e6.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e7=e7.reset_index().drop(['match','level_0'],axis=1)\n",
    "    df2.index.name= 'key'\n",
    "    e1.index.name = 'key'\n",
    "    e2.index.name = 'key'\n",
    "    e3.index.name = 'key'\n",
    "    e4.index.name = 'key'\n",
    "    e5.index.name = 'key'\n",
    "    e6.index.name = 'key'\n",
    "    e7.index.name = 'key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all=df2.merge(e1,how='left',on='key').merge(e2,\n",
    "        how='left',on='key').merge(e4, \n",
    "        how='left',on='key').merge(e5, \n",
    "        how='left',on='key').merge(e6, \n",
    "        how='left',on='key').merge(e7, \n",
    "        how='left',on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3d7a75bee929>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0me6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'key'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0me7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'key'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m all=df2.reset_index().merge(e1.reset_index().drop(['match'],axis=1),how='left',on='key').merge(e2.reset_index().drop(['match'],axis=1),\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'match'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'match'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   6387\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6388\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6389\u001b[1;33m                      copy=copy, indicator=indicator, validate=validate)\n\u001b[0m\u001b[0;32m   6390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     59\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    549\u001b[0m         (self.left_join_keys,\n\u001b[0;32m    550\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    855\u001b[0m                             right_keys.append(\n\u001b[0;32m    856\u001b[0m                                 right._get_label_or_level_values(\n\u001b[1;32m--> 857\u001b[1;33m                                     rk, stacklevel=stacklevel))\n\u001b[0m\u001b[0;32m    858\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis, stacklevel)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'key'"
     ]
    }
   ],
   "source": [
    "    import numpy as np\n",
    "    df2=pd.DataFrame(df, columns=['text'])\n",
    "\n",
    "    #Search for dates\n",
    "    #04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "    e1=df2['text'].str.extractall(r'(?P<month>(?:\\d{1,2}))/(?P<day>(?:\\d{1,2}))/(?P<year>(?:\\d{4}|\\d{2}))')\n",
    "    e1=e1.loc[pd.to_numeric(e1.day)<=31,:]\n",
    "    #Mar-20-2009; Mar 20, 2009; March 20, 2009; Mar. 20, 2009; Mar 20 2009;\n",
    "    e2=df2['text'].str.extractall(r'(?P<month2>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z.,\\- ]*(?P<day2>(?:\\d{1,2}))[,]* (?P<year2>(?:\\d{4}))')\n",
    "    #Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "    e3=df2['text'].str.extractall(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{1,2})(st|nd|th)+[, ]?(?:\\d{2,4})')\n",
    "    #Feb 2009; Sep 2009; Oct 2010\n",
    "    e4=df2['text'].str.extractall(r'(?P<month4>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)) (?P<year4>(?:\\d{2,4}))')\n",
    "    #6/2008; 12/2009\n",
    "    e5=df2['text'].str.extractall(r'(?P<month5>(?:[1][0-2]|\\d{1}))/(?P<year5>(?:\\d{4}))')\n",
    "    #2009; 2010\n",
    "    e6=df2['text'].str.extractall(r'(?P<year6>(?:1|2)\\d{3})')\n",
    "    #20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "    e7=df2['text'].str.extractall(r'(?P<e7>(?P<day7>(?:\\d{1,2}) )?(?P<month7>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z., ]*(?P<year7>(?:\\d{4})))')\n",
    "\n",
    "    #merge above\n",
    "    e1=e1.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e2=e2.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e4=e4.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e5=e5.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e6=e6.reset_index().drop(['match','level_0'],axis=1)\n",
    "    e7=e7.reset_index().drop(['match','level_0'],axis=1)\n",
    "    df2.index.name= 'key'\n",
    "    e1.index.name = 'key'\n",
    "    e2.index.name = 'key'\n",
    "    e3.index.name = 'key'\n",
    "    e4.index.name = 'key'\n",
    "    e5.index.name = 'key'\n",
    "    e6.index.name = 'key'\n",
    "    e7.index.name = 'key'\n",
    "    all=df2.merge(e1,how='left',on='key').merge(e2,\n",
    "        how='left',on='key').merge(e4, \n",
    "        how='left',on='key').merge(e5, \n",
    "        how='left',on='key').merge(e6, \n",
    "        how='left',on='key').merge(e7, \n",
    "        how='left',on='key')\n",
    "    #impute letter months to numbers\n",
    "    month_dict={'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "    all.replace(dict(month=month_dict, month2=month_dict,  month4=month_dict, month7=month_dict), inplace=True)\n",
    "\n",
    "    #impute missing year, month, and day from e1\n",
    "    all.month.fillna(all.month7, inplace=True)\n",
    "    all.month.fillna(all.month2, inplace=True)\n",
    "    all.month.fillna(all.month4, inplace=True)\n",
    "    all.month.fillna(all.month5, inplace=True)\n",
    "    all.day.fillna(all.day7, inplace=True)\n",
    "    all.day.fillna(all.day2, inplace=True)\n",
    "    all.year.fillna(all.year7, inplace=True)\n",
    "    all.year.fillna(all.year2, inplace=True)\n",
    "    all.year.fillna(all.year4, inplace=True)\n",
    "    all.year.fillna(all.year5, inplace=True)\n",
    "    all.year.fillna(all.year6, inplace=True)\n",
    "    all1=all[['text','month', 'day', 'year']]\n",
    "    all2=all1.loc[:,('month', 'day', 'year')].apply(pd.to_numeric)\n",
    "\n",
    "    #impute missing month and day with 1\n",
    "    values = {'month': 1, 'day': 1}\n",
    "    all3=all2.fillna(value=values)\n",
    "\n",
    "    #convert 2 letter year to 1990's\n",
    "    all3.year=np.where(all3.year<1900,all3.year+1900,all3.year)\n",
    "\n",
    "    #sort by date\n",
    "    all3=all3.sort_values(by=['year','month','day'],na_position='last')\n",
    "    all4=all3.drop(['month','day','year'],axis=1).reset_index()\n",
    "    pd.Series(all4['key'], index=all4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_sorter():\n",
    "    import numpy as np\n",
    "    df2=pd.DataFrame(df, columns=['text'])\n",
    "\n",
    "    #Search for dates\n",
    "    #04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "    e1=df2['text'].str.extractall(r'(?P<month>(?:\\d{1,2}))/(?P<day>(?:\\d{1,2}))/(?P<year>(?:\\d{4}|\\d{2}))')\n",
    "    e1=e1.loc[pd.to_numeric(e1.day)<=31,:]\n",
    "    #Mar-20-2009; Mar 20, 2009; March 20, 2009; Mar. 20, 2009; Mar 20 2009;\n",
    "    e2=df2['text'].str.extractall(r'(?P<month2>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z.,\\- ]*(?P<day2>(?:\\d{1,2}))[,]* (?P<year2>(?:\\d{4}))')\n",
    "    #Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "    e3=df2['text'].str.extractall(r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{1,2})(st|nd|th)+[, ]?(?:\\d{2,4})')\n",
    "    #Feb 2009; Sep 2009; Oct 2010\n",
    "    e4=df2['text'].str.extractall(r'(?P<month4>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)) (?P<year4>(?:\\d{2,4}))')\n",
    "    #6/2008; 12/2009\n",
    "    e5=df2['text'].str.extractall(r'(?P<month5>(?:[1][0-2]|\\d{1}))/(?P<year5>(?:\\d{4}))')\n",
    "    #2009; 2010\n",
    "    e6=df2['text'].str.extractall(r'(?P<year6>(?:1|2)\\d{3})')\n",
    "    #20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "    e7=df2['text'].str.extractall(r'(?P<e7>(?P<day7>(?:\\d{1,2}) )?(?P<month7>(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))[a-z., ]*(?P<year7>(?:\\d{4})))')\n",
    "\n",
    "    #merge above\n",
    "    df2.index.name= 'key'\n",
    "    e1.index.name = 'key'\n",
    "    e2.index.name = 'key'\n",
    "    e3.index.name = 'key'\n",
    "    e4.index.name = 'key'\n",
    "    e5.index.name = 'key'\n",
    "    e6.index.name = 'key'\n",
    "    e7.index.name = 'key'\n",
    "    all=df2.reset_index().merge(e1.reset_index().drop(['match'],axis=1),how='left',on='key').merge(e2.reset_index().drop(['match'],axis=1),\n",
    "        how='left',on='key').merge(e4.reset_index().drop(['match'],axis=1), \n",
    "        how='left',on='key').merge(e5.reset_index().drop(['match'],axis=1), \n",
    "        how='left',on='key').merge(e6.reset_index().drop(['match'],axis=1), \n",
    "        how='left',on='key').merge(e7.reset_index().drop(['match'],axis=1), \n",
    "        how='left',on='key')\n",
    "    #impute letter months to numbers\n",
    "    month_dict={'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "    all.replace(dict(month=month_dict, month2=month_dict,  month4=month_dict, month7=month_dict), inplace=True)\n",
    "\n",
    "    #impute missing year, month, and day from e1\n",
    "    all.month.fillna(all.month7, inplace=True)\n",
    "    all.month.fillna(all.month2, inplace=True)\n",
    "    all.month.fillna(all.month4, inplace=True)\n",
    "    all.month.fillna(all.month5, inplace=True)\n",
    "    all.day.fillna(all.day7, inplace=True)\n",
    "    all.day.fillna(all.day2, inplace=True)\n",
    "    all.year.fillna(all.year7, inplace=True)\n",
    "    all.year.fillna(all.year2, inplace=True)\n",
    "    all.year.fillna(all.year4, inplace=True)\n",
    "    all.year.fillna(all.year5, inplace=True)\n",
    "    all.year.fillna(all.year6, inplace=True)\n",
    "    all1=all[['text','month', 'day', 'year']]\n",
    "    all2=all1.loc[:,('month', 'day', 'year')].apply(pd.to_numeric)\n",
    "\n",
    "    #impute missing month and day with 1\n",
    "    values = {'month': 1, 'day': 1}\n",
    "    all3=all2.fillna(value=values)\n",
    "\n",
    "    #convert 2 letter year to 1990's\n",
    "    all3.year=np.where(all3.year<1900,all3.year+1900,all3.year)\n",
    "\n",
    "    #sort by date\n",
    "    all3=all3.sort_values(by=['year','month','day'],na_position='last')\n",
    "    all4=all3.drop(['month','day','year'],axis=1).reset_index()\n",
    "    \n",
    "    \n",
    "    return pd.Series(all4['key'], index=all4.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9\n",
       "1       80\n",
       "2        2\n",
       "3       50\n",
       "4       27\n",
       "5       13\n",
       "6       94\n",
       "7      107\n",
       "8       30\n",
       "9       35\n",
       "10      54\n",
       "11     100\n",
       "12      91\n",
       "13      69\n",
       "14     104\n",
       "15      78\n",
       "16      46\n",
       "17      11\n",
       "18       3\n",
       "19      47\n",
       "20      23\n",
       "21      26\n",
       "22      89\n",
       "23      17\n",
       "24     119\n",
       "25      19\n",
       "26     113\n",
       "27      68\n",
       "28     101\n",
       "29       6\n",
       "      ... \n",
       "470    470\n",
       "471    471\n",
       "472    472\n",
       "473    473\n",
       "474    474\n",
       "475    475\n",
       "476    476\n",
       "477    477\n",
       "478    478\n",
       "479    479\n",
       "480    480\n",
       "481    481\n",
       "482    482\n",
       "483    483\n",
       "484    484\n",
       "485    485\n",
       "486    486\n",
       "487    487\n",
       "488    488\n",
       "489    489\n",
       "490    490\n",
       "491    491\n",
       "492    492\n",
       "493    493\n",
       "494    494\n",
       "495    495\n",
       "496    496\n",
       "497    497\n",
       "498    498\n",
       "499    499\n",
       "Name: key, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_sorter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "LvcWI",
   "launcher_item_id": "krne9",
   "part_id": "Mkp1I"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
